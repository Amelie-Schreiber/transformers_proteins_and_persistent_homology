{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Persistent Homology Dimension of Protein Sequences in the Latent Space of ESM-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an attempt at an implementation of the estimation of Persistent Homology Dimension described in [Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts](https://arxiv.org/abs/2306.04723). This is a fractal dimension that aims to detect protein sequences generated by AI. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. **Model and Tokenizer Initialization**: An ESM-2 protein model and tokenizer are initialized. These represent the mapping function $ f: M \\rightarrow \\mathbb{R}^n $.\n",
    "\n",
    "2. **Protein Sequence Input and Tokenization**: A protein sequence, which can be thought of as a finite subset $ X \\subseteq M $, is provided as input and tokenized. The tokens are then converted to tensors, which are the required input format for the ESM model.\n",
    "\n",
    "3. **Embedding Generation**: The token tensors are passed through the model $ f $ to generate a set of embeddings $ Y = f(X) $. Each embedding is a point in $ \\mathbb{R}^n $.\n",
    "\n",
    "4. **Embedding Subsetting**: The embeddings corresponding to the first and last tokens are removed. These special tokens do not represent actual amino acids in the protein sequence and so are not part of the subset $ X $.\n",
    "\n",
    "5. **Subsampling, Distance Matrix Calculation, and MST Calculation**: A series of subsets $ S_i \\subseteq Y $, $i = 1, \\ldots, k $ are created, with sizes $ n_i $ varying from 2 to $ |Y| $. For each subset $ S_i $, a distance matrix is calculated and used to compute the minimum spanning tree (MST). The MST corresponds to a graph $ G \\subseteq Y $.\n",
    "\n",
    "6. **Persistent Score Calculation**: The persistent score $E_0^\\alpha(S_i) $ is calculated as the maximum edge length in the MST, which corresponds to the lifespans of 0-dimensional features in the PH computation. In this case, $ \\alpha = 1 $.\n",
    "\n",
    "7. **Data Preparation**: The sizes of the subsets and the corresponding persistent scores are logged (natural logarithm) and stored for linear regression.\n",
    "\n",
    "8. **Linear Regression**: Linear regression is performed on the log-transformed sizes and persistent scores to approximate the relationship $ \\log E_0^\\alpha(S_i) \\sim (1 - \\frac{\\alpha}{d}) \\log n_i $. The slope of the regression line is then used to calculate the Persistent Homology Dimension (PHD) using the formula $ d = \\frac{1}{1 - \\text{reg.coef}[0][0]} $, which corresponds to $ d = \\text{dim}_0^{PH}(M) $.\n",
    "\n",
    "In summary, the code is estimating the 0-dimensional Persistent Homology Dimension of the manifold $M$ represented by the protein sequence. This is achieved by generating embeddings for the sequence, calculating the MST for subsets of these embeddings, and then performing linear regression on the sizes of these subsets and their corresponding persistent scores. It is unclear whether or not there is any difference in Persistent Homology Dimension for \"Natural Proteins\" vs. \"AI Proteins\", so this is just an initial exploratory notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing EsmModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.weight', 'esm.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0783164527027296\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from transformers import AutoTokenizer, AutoModel, EsmModel\n",
    "import torch\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "model = EsmModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "\n",
    "# Input text\n",
    "text = \"MAPLRKTYVLKLYVAGNTPNSVRALKTLNNILEKEFKGVYALKVIDVLKNPQLAEEDKILATPTLAKVLPPPVRRIIGDLSNREKVLIGLDLLYEEIGDQAEDDLGLE\"\n",
    "\n",
    "# Tokenize the input and convert to tensors\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[0].numpy()\n",
    "\n",
    "# Remove the first and last embeddings (<CLS> and <EOS>)\n",
    "embeddings = embeddings[1:-1]\n",
    "\n",
    "# Sizes for the subsets to sample\n",
    "sizes = np.linspace(2, len(embeddings), num=100, dtype=int)\n",
    "\n",
    "# Prepare data for linear regression\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for size in sizes:\n",
    "    # Sample a subset of the embeddings\n",
    "    subset = np.random.choice(len(embeddings), size, replace=False)\n",
    "    subset_embeddings = embeddings[subset]\n",
    "    \n",
    "    # Compute the distance matrix\n",
    "    dist_matrix = np.sqrt(np.sum((subset_embeddings[:, None] - subset_embeddings)**2, axis=-1))\n",
    "\n",
    "    # Compute the minimum spanning tree\n",
    "    mst = minimum_spanning_tree(dist_matrix).toarray()\n",
    "\n",
    "    # Calculate the persistent score E (the maximum edge length in the MST)\n",
    "    E = np.max(mst)\n",
    "\n",
    "    # Append to the data for linear regression\n",
    "    x.append(np.log(size))\n",
    "    y.append(np.log(E))\n",
    "\n",
    "# Reshape for sklearn\n",
    "X = np.array(x).reshape(-1, 1)\n",
    "Y = np.array(y).reshape(-1, 1)\n",
    "\n",
    "# Linear regression\n",
    "reg = LinearRegression().fit(X, Y)\n",
    "\n",
    "# Estimated Persistent Homology Dimension\n",
    "phd = 1 / (1 - reg.coef_[0][0])\n",
    "print(phd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
